{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is meant to help a developer run autodeploy. You would need to run through all the cells to get your first autodeploy'ed' model running\n",
    "\n",
    "### Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported OS - OSx, Any version of Linux(Ubuntu preferred), Windows 10  \n",
    "### A linux VM on AWS should get you going quickly \n",
    "### Prerequisites - Ensure you have the following softwares installed on your system\n",
    "\n",
    "- Install docker \n",
    "  - For Ubuntu (and Linux distros) - [Install Docker on Ubuntu](https://docs.docker.com/engine/install/ubuntu/#installation-methods)  \n",
    "  - For Windows - [Install Docker on Windows](https://docs.docker.com/desktop/windows/install/)\n",
    "  - For Mac - \n",
    "\n",
    "- Install docker-compose\n",
    "  - For Ubuntu (and Linux distros) - [Install docker-compose on Linux](https://docs.docker.com/compose/install/)\n",
    "  - For Windows and Mac\n",
    " \n",
    "- Install git : https://git-scm.com/book/en/v2/Getting-Started-Installing-Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Post model, issues that a developer faces.\n",
    "- Demonstrated standardization for autotrain with a config.\n",
    "- Now we will see how we can standardize/demonstrate autodeploy through configuration - a low code approach.\n",
    "\n",
    "**What is autodeploy.**\n",
    " - What are the various components in autodeploy\n",
    "  - Optimizing model for edge deployment - coversion to onnx, tflite\n",
    "  - model configurator and loader\n",
    "  - FastAPI Interface\n",
    "  - Moitoring server (Enabled with rabbitmq and prometheus)\n",
    "  - request store - SQlLite\n",
    "  - Metrics visualization : grafana\n",
    "\n",
    "**Image Classification**\n",
    "Run through of the configuration file sections\n",
    " - path model deployment artefacts \n",
    " - model name and input out shapes \n",
    " - schema of the input/output FastAPI (conf. param is mandatory)\n",
    " - defining the preprocess/process functions\n",
    " - defining model and service monitoring metrics \n",
    "\n",
    "**What are the various containers that are orchestrated in this process**\n",
    " - No need to tinker with the architecture/design. Absracted\n",
    " - autodeploy (with FastAPI) - ***\n",
    " - rabbitmq - queues up request/responses\n",
    " - prometheus - captures the metrics\n",
    " - grafana - visualization of the model and service metrics\n",
    "\n",
    "**Vision**\n",
    "- Detection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**... best practices incorporated that a dev doesnt have to tinker repeatedly/\n",
    "\n",
    "\n",
    "\n",
    "Good to have\n",
    "- map SQLLite volume external to the monitor container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AutoDeploy'...\n",
      "remote: Enumerating objects: 841, done.\u001b[K\n",
      "remote: Counting objects: 100% (841/841), done.\u001b[K\n",
      "remote: Compressing objects: 100% (521/521), done.\u001b[K\n",
      "remote: Total 841 (delta 444), reused 617 (delta 255), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (841/841), 33.32 MiB | 14.78 MiB/s, done.\n",
      "Resolving deltas: 100% (444/444), done.\n",
      "Branch 'dev' set up to track remote branch 'dev' from 'origin'.\n",
      "Switched to a new branch 'dev'\n"
     ]
    }
   ],
   "source": [
    "# Clone the repo \n",
    "!git clone https://github.com/kartik4949/AutoDeploy.git\n",
    "\n",
    "# Changing over to the AutoDeploy directory\n",
    "os.chdir('AutoDeploy') \n",
    "\n",
    "!git checkout dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-14 19:12:48--  https://github.com/kartik4949/AutoDeploy/files/7159611/model_dependencies.zip\n",
      "Resolving github.com (github.com)... 13.234.210.38\n",
      "Connecting to github.com (github.com)|13.234.210.38|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-repository-files.githubusercontent.com/394000201/7159611?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210914%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210914T134249Z&X-Amz-Expires=300&X-Amz-Signature=07d50e94416cc6a05d659207c1f7d4bfb6fe7480b979d3fb1412799c8875262d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=394000201&response-content-disposition=attachment%3Bfilename%3Dmodel_dependencies.zip&response-content-type=application%2Fzip [following]\n",
      "--2021-09-14 19:12:49--  https://github-repository-files.githubusercontent.com/394000201/7159611?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210914%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210914T134249Z&X-Amz-Expires=300&X-Amz-Signature=07d50e94416cc6a05d659207c1f7d4bfb6fe7480b979d3fb1412799c8875262d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=394000201&response-content-disposition=attachment%3Bfilename%3Dmodel_dependencies.zip&response-content-type=application%2Fzip\n",
      "Resolving github-repository-files.githubusercontent.com (github-repository-files.githubusercontent.com)... 185.199.109.154, 185.199.108.154, 185.199.111.154, ...\n",
      "Connecting to github-repository-files.githubusercontent.com (github-repository-files.githubusercontent.com)|185.199.109.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14886329 (14M) [application/zip]\n",
      "Saving to: ‘model_dependencies.zip’\n",
      "\n",
      "model_dependencies. 100%[===================>]  14.20M  9.13MB/s    in 1.6s    \n",
      "\n",
      "2021-09-14 19:12:51 (9.13 MB/s) - ‘model_dependencies.zip’ saved [14886329/14886329]\n",
      "\n",
      "Archive:  model_dependencies.zip\n",
      "   creating: model_dependencies/\n",
      "  inflating: model_dependencies/horse_zebra.onnx  \n",
      "  inflating: model_dependencies/preprocess.py  \n",
      "  inflating: model_dependencies/postprocess.py  \n",
      "  inflating: model_dependencies/custom_metrics.py  \n",
      "  inflating: model_dependencies/requirements.txt  \n",
      "  inflating: model_dependencies/structured_ref.npy  \n",
      "The model and the dependencies are : \n",
      "custom_metrics.py  postprocess.py  requirements.txt\n",
      "horse_zebra.onnx   preprocess.py   structured_ref.npy\n"
     ]
    }
   ],
   "source": [
    "# Get a sample onnx model (image classification) and examples of dependent python files (We will discuss each of them later)\n",
    "# Extract the repo\n",
    "!wget https://github.com/kartik4949/AutoDeploy/files/7159611/model_dependencies.zip\n",
    "!unzip model_dependencies.zip\n",
    "\n",
    "print(\"The model and the dependencies are : \")\n",
    "!ls model_dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You would see the following files in the Autodeploy/model_dependencies folder\n",
    "- horse_zebra.onnx - The onnx model file for image classification between horses and zebras\n",
    "- custom_metrics.py - File that contains the metrics to capture\n",
    "- postprocess.py - This file contains and data post processing steps\n",
    "- preprocess.py - This contains any data preprocessing steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your configuration file. Lets go through the main sections of the config file. More info on each of the paramaters may be found [here](https://github.com/kartik4949/AutoDeploy/wiki/4.-Setup-configuration-file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The directory where the model dependencies would be stored**\n",
    "```\n",
    "dependency:\n",
    "        path: '/app/model_dependencies'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model info and input/output parameters**\n",
    "\n",
    "```\n",
    "model:\n",
    "        model_type: 'onnx' \n",
    "        model_path: 'horse_zebra.onnx' \n",
    "        model_file_type: 'onnx'\n",
    "        version: '1.0.0'\n",
    "        model_name: 'computer vision classification model.'\n",
    "        endpoint: 'predict' \n",
    "        protected: 0\n",
    "        input_type: 'serialized' #used when array is being passed (typically dl models)\n",
    "        input_shape: [224, 224, 3] #only used if the data type is serialized\n",
    "        server:\n",
    "               name: 'autodeploy'\n",
    "               port: 8000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preprocess and post process function names**\n",
    "```\n",
    "preprocess: 'custom_preprocess_classification'\n",
    "postprocess: 'custom_postprocess'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input and output schema (pydantic models) for API endpoints**\n",
    "\n",
    "```\n",
    "input_schema:\n",
    "        input: 'string'\n",
    "out_schema:\n",
    "        out: 'int'\n",
    "        confidence: 'float'\n",
    "        status: 'int'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Monitoring server parameters - model metrics and service metrics**\n",
    "```\n",
    "monitor:\n",
    "        server:\n",
    "                name: 'rabbitmq'\n",
    "                port: 5672\n",
    "        custom_metrics: 'image_brightness'\n",
    "        metrics:\n",
    "                average_per_day:\n",
    "                        type: 'info'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First he/she would need to define the date preprocessing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/classification/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/classification/config.yaml\n",
    "model:\n",
    "        model_type: 'onnx'\n",
    "        model_path: ['horse_zebra.onnx', 'horse_zebra.onnx']\n",
    "        ab_split: [80,20]\n",
    "        model_file_type: 'onnx'\n",
    "        version: '1.0.0'\n",
    "        model_name: 'computer vision classification model.'\n",
    "        endpoint: 'predict'\n",
    "        protected: 0\n",
    "        input_type: 'serialized'\n",
    "        input_shape: [224, 224, 3]\n",
    "        server:\n",
    "                name: 'autodeploy'\n",
    "                port: 8000\n",
    "preprocess: 'custom_preprocess_classification'\n",
    "postprocess: 'custom_postprocess'\n",
    "input_schema:\n",
    "        input: 'string'\n",
    "out_schema:\n",
    "        out: 'int'\n",
    "        confidence: 'float'\n",
    "        status: 'int'\n",
    "dependency:\n",
    "        path: '/app/model_dependencies'\n",
    "monitor:\n",
    "        server:\n",
    "                name: 'rabbitmq'\n",
    "                port: 5672        \n",
    "        custom_metrics: 'image_brightness'\n",
    "        metrics:\n",
    "                average_per_day:\n",
    "                        type: 'info'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the data scientist would need to create a set of 3 files before deploying the model successfuly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_dependencies/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_dependencies/preprocess.py\n",
    "from register import PREPROCESS\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch \n",
    "\n",
    "def get_transformed_image(img):\n",
    "  p_im = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "  tf = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "  return tf(p_im)    \n",
    "\n",
    "@PREPROCESS.register_module(name='custom_preprocess_classification')\n",
    "def custom_preprocess_fxn(input):\n",
    "  print(f\"The shape is {np.array(input).shape}\")\n",
    "  pp_input = get_transformed_image(input[0])\n",
    "  return np.array(pp_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then define the post processing function. This is how the model output will be processed to be presented to the end user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_dependencies/postprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_dependencies/postprocess.py\n",
    "from register import POSTPROCESS\n",
    "from scipy.special import softmax\n",
    "\n",
    "@POSTPROCESS.register_module(name='custom_postprocess')\n",
    "def custom_postprocess_fxn(output):\n",
    "  confidences = softmax(output).max(-1)\n",
    "  classes = softmax(output).argmax(-1)\n",
    "  print(classes, confidences)\n",
    "  output = {'out': classes.tolist()[0],\n",
    "            'confidence': confidences.tolist()[0],\n",
    "            'status': 200}\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the file which contains the custom model metrics to log to prometheus - for tracking in Grafana (Needs more work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_dependencies/custom_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_dependencies/custom_metrics.py\n",
    "from register import METRICS\n",
    "import cv2\n",
    "\n",
    "@METRICS.register_module(name=\"image_brightness\")\n",
    "def get_brightness(image, dim=10, thresh=0.5):\n",
    "    # Resize image to 10x10\n",
    "    image = cv2.resize(image, (dim, dim))\n",
    "    # Convert color space to LAB format and extract L channel\n",
    "    L, A, B = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2LAB))\n",
    "    # Normalize L channel by dividing all pixel values with maximum pixel value\n",
    "    L = L/np.max(L)\n",
    "    # Return True if mean is greater than thresh else False\n",
    "    return np.mean(L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thats all the files that would need to be configured/coded. Let's now build the docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dependencies/requirements.txt\n",
      "Sending build context to Docker daemon  90.42MB\n",
      "Step 1/19 : FROM ubuntu:20.04\n",
      " ---> fb52e22af1b0\n",
      "Step 2/19 : ARG MODEL_REQ\n",
      " ---> Using cache\n",
      " ---> 6c31360f6e9f\n",
      "Step 3/19 : RUN apt-get update     && apt-get install python3 python3-pip -y     && apt-get clean     && apt-get autoremove\n",
      " ---> Using cache\n",
      " ---> f6b056842ea1\n",
      "Step 4/19 : ENV TZ=Europe/Kiev\n",
      " ---> Using cache\n",
      " ---> 15429b34be6c\n",
      "Step 5/19 : RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n",
      " ---> Using cache\n",
      " ---> bbe1c43884aa\n",
      "Step 6/19 : RUN apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender1 libxext6 -y\n",
      " ---> Using cache\n",
      " ---> 710a24730003\n",
      "Step 7/19 : RUN apt-get install iputils-ping netcat -y\n",
      " ---> Using cache\n",
      " ---> 8b9a3285129f\n",
      "Step 8/19 : COPY ./requirements.txt /app/requirements.txt\n",
      " ---> Using cache\n",
      " ---> 12fae4bb6690\n",
      "Step 9/19 : RUN python3 -m pip install -r /app/requirements.txt\n",
      " ---> Using cache\n",
      " ---> cfc7529e2abf\n",
      "Step 10/19 : COPY $MODEL_REQ /app/$MODEL_REQ\n",
      " ---> Using cache\n",
      " ---> 99ba7cac5a7f\n",
      "Step 11/19 : RUN python3 -m pip install -r /app/$MODEL_REQ\n",
      " ---> Using cache\n",
      " ---> 91e456ff3cbf\n",
      "Step 12/19 : ENV LC_ALL=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 310569580b83\n",
      "Step 13/19 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 9a33dfd458af\n",
      "Step 14/19 : ENV LANGUAGE=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 55394ed9a418\n",
      "Step 15/19 : EXPOSE 8000\n",
      " ---> Using cache\n",
      " ---> 9d846f42a60c\n",
      "Step 16/19 : COPY ./ app\n",
      " ---> 5fb61b2a7f64\n",
      "Step 17/19 : WORKDIR /app\n",
      " ---> Running in 6a2346cdd7ac\n",
      "Removing intermediate container 6a2346cdd7ac\n",
      " ---> d32ee448c96b\n",
      "Step 18/19 : RUN chmod +x /app/autodeploy_start.sh\n",
      " ---> Running in 246ec0cabd9b\n",
      "Removing intermediate container 246ec0cabd9b\n",
      " ---> 5d6fe8965cfe\n",
      "Step 19/19 : CMD [\"/app/autodeploy_start.sh\"]\n",
      " ---> Running in de77d1baaa37\n",
      "Removing intermediate container de77d1baaa37\n",
      " ---> f25ae4569415\n",
      "Successfully built f25ae4569415\n",
      "Successfully tagged autodeploy:latest\n",
      "Sending build context to Docker daemon  90.42MB\n",
      "Step 1/19 : FROM ubuntu:20.04\n",
      " ---> fb52e22af1b0\n",
      "Step 2/19 : ARG MODEL_REQ\n",
      " ---> Using cache\n",
      " ---> 6c31360f6e9f\n",
      "Step 3/19 : RUN apt-get update     && apt-get install python3 python3-pip -y     && apt-get clean     && apt-get autoremove\n",
      " ---> Using cache\n",
      " ---> f6b056842ea1\n",
      "Step 4/19 : ENV TZ=Europe/Kiev\n",
      " ---> Using cache\n",
      " ---> 15429b34be6c\n",
      "Step 5/19 : RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n",
      " ---> Using cache\n",
      " ---> bbe1c43884aa\n",
      "Step 6/19 : RUN apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender1 libxext6 -y\n",
      " ---> Using cache\n",
      " ---> 710a24730003\n",
      "Step 7/19 : RUN apt-get install iputils-ping netcat -y\n",
      " ---> Using cache\n",
      " ---> 8b9a3285129f\n",
      "Step 8/19 : COPY ./requirements.txt /app/requirements.txt\n",
      " ---> Using cache\n",
      " ---> 12fae4bb6690\n",
      "Step 9/19 : RUN python3 -m pip install -r /app/requirements.txt\n",
      " ---> Using cache\n",
      " ---> cfc7529e2abf\n",
      "Step 10/19 : COPY $MODEL_REQ /app/$MODEL_REQ\n",
      " ---> Using cache\n",
      " ---> 99ba7cac5a7f\n",
      "Step 11/19 : RUN python3 -m pip install -r /app/$MODEL_REQ\n",
      " ---> Using cache\n",
      " ---> 91e456ff3cbf\n",
      "Step 12/19 : ENV LC_ALL=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 310569580b83\n",
      "Step 13/19 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 9a33dfd458af\n",
      "Step 14/19 : ENV LANGUAGE=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 55394ed9a418\n",
      "Step 15/19 : EXPOSE 8001\n",
      " ---> Using cache\n",
      " ---> 2b10c5907f26\n",
      "Step 16/19 : COPY ./ app\n",
      " ---> c4d09f532da8\n",
      "Step 17/19 : WORKDIR /app\n",
      " ---> Running in 47b3ea180d39\n",
      "Removing intermediate container 47b3ea180d39\n",
      " ---> b731f45c9a24\n",
      "Step 18/19 : RUN chmod +x /app/monitor_start.sh\n",
      " ---> Running in dd48b6f7de17\n",
      "Removing intermediate container dd48b6f7de17\n",
      " ---> 49c47339dc29\n",
      "Step 19/19 : CMD [\"/app/monitor_start.sh\"]\n",
      " ---> Running in 3fc4a28df632\n",
      "Removing intermediate container 3fc4a28df632\n",
      " ---> 82239157e57f\n",
      "Successfully built 82239157e57f\n",
      "Successfully tagged monitor:latest\n"
     ]
    }
   ],
   "source": [
    "# Call build.sh passing in the custom requirements.txt file which contains the package dependencies used in pre/postprocess and \n",
    "# metrics\n",
    "!./build.sh -r model_dependencies/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Define your autodeploy configuration path in docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: \"2.1\"\n",
    "\n",
    "services:\n",
    "  rabbitmq:\n",
    "    image: rabbitmq:3-management\n",
    "    restart: always\n",
    "    ports:\n",
    "      - \"15672:15672\"\n",
    "      - \"5672:5672\"\n",
    "\n",
    "  autodeploy:\n",
    "    image: autodeploy:latest\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    links: \n",
    "      - rabbitmq\n",
    "    networks:\n",
    "      - default\n",
    "    environment: \n",
    "      - CONFIG=/app/${CONFIG}\n",
    "\n",
    "  monitor:\n",
    "    image: monitor:latest\n",
    "    ports:\n",
    "      - \"8001:8001\"\n",
    "    links: \n",
    "      - rabbitmq\n",
    "    networks:\n",
    "      - default\n",
    "    environment: \n",
    "      - CONFIG=/app/${CONFIG}\n",
    "  \n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    links: \n",
    "      - rabbitmq\n",
    "      - autodeploy\n",
    "    volumes: \n",
    "      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    links: \n",
    "      - prometheus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Thats it. Start up your autodeploy server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./start.sh -f configs/classification/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to [http://localhost:8000/docs](http://localhost:8000/docs) and your model is ready to be served :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your model with the following requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/DATA/Learning/ODSC/AutoDeploy/notebooks/horse_1.jpg\", mode='rb') as f:\n",
    "    im = Image.open(f).convert('RGB')\n",
    "\n",
    "im\n",
    "# im = np.array([np.array(im)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the prediction endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.array(im)\n",
    "url=\"http://localhost:8000/predict\"\n",
    "data = {\n",
    "  \"input\": json.dumps(im.tolist())\n",
    "}\n",
    "requests.post(url, json=data).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "sess = ort.InferenceSession('AutoDeploy/model_dependencies/horse_zebra.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "pred_onx = sess.run([label_name], {input_name: np.array([im], dtype=np.float32)})[0]\n",
    "print(pred_onx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a8ebe6ca3f7433956efd5e4dcbc09435871f279559c06d4aeaafe36d167141"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
